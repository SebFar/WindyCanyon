#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Mon Jul 10 14:47:11 2017

@author: Sebastian Farquhar

Comments on coding style welcome and can be sent to sebfar @ gmail [dot] com

Code sets up the 'Windy Canyon' environment from Sutton and Barto Chapter 6
and implements a SARSA, Q-learning, and ESARSA algorithm to learn the optimal
path.

Important variables:
    q is a 7x10x9 matrix of mapHeight, mapWidth, actions
    canyonMap is 7x10 matrix of the windspeeds
    actions are integers 1-9 where first game uses 1-4, second 1-8, last 1-9
functions:
    takeStep moves the agent by one action and applies the wind:
        x, y, action, stochastic, canyonMap -> x, y, reward, terminate
    chooseAction picks an epsilon-greedy action:
        x, y, q, epsilon, numberOfActions -> action
    SARSAEV finds the expectation of the next step for ESARSA:
        x, y, q, epsilon, numberOfActions -> expectation
    doARun performs a complete learning loop depending on which learner is asked for:
        episodeMax, discountFactor, q, numberOfActions, stochastic, typeOfAlgorithm -> scores
    findPathName helps the data savers and display functions get the right filenames
        numberOfActions, stochastic -> file
    dataSaver appends the most recent run to file for later analysis
        scores, numberOfActions, stochastic, typeOfAlgorithm -> True
    dataAggregator Returns a score array averaged over the different runs recorded for this configuration
        numberOfActions, stochastic -> averagedScores
    graphDisplay helps show each subplot
        stochastic, numberOfActions, position, startGraphs, title -> True
    dataDisplay unpickles, averages types of runs, and displays graphs
    
    Main body cycles through configurations and then displays and saves the graphs
"""
import numpy as np
import random
import math
import os.path
import matplotlib.pyplot as plt


def takeStep(xIn, yIn, action, stochastic, canyonMap):
    "takeStep moves the agent by one action and applies the wind"
    x, y = xIn, yIn
    if action == 0:  # up
        y += 1
    elif action == 1:  # down
        y -= 1
    elif action == 2:  # right
        x += 1
    elif action == 3:  # left
        x -= 1
    elif action == 4:  # lu
        x -= 1
        y += 1
    elif action == 5:  # ur
        x += 1
        y += 1
    elif action == 6:  # dr
        x += 1
        y -= 1
    elif action == 7:  # dl
        x -= 1
        y -= 1
    # action == 8 does nothing
 
    # move the agent back onto the map if it's gone off
    if x >= canyonMap.shape[1]:
        x = canyonMap.shape[1] - 1
    if x < 0:
        x = 0
    if y >= canyonMap.shape[0]:
        y = canyonMap.shape[0] - 1
    if y < 0:
        y = 0

    # apply the wind
    y += canyonMap[y, x]
    
    if stochastic:
        windiness = random.random()
        if windiness < (1/3):
            y += 1
        elif windiness < (2/3):
            y -= 1
            
    #now check the y edge again
    if y >= canyonMap.shape[0]:
        y = canyonMap.shape[0] - 1
    if y < 0:
        y = 0
    
    # set the reward
    reward = -1
    terminate = False
    if (x == 7) and (y == 3):
        reward = 0
        terminate = True
    
    return [x, y, reward, terminate]


def chooseAction(x, y, q, epsilon, numberOfActions):
    "chooseAction picks an epsilon-greedy action"
    if random.random() > epsilon:
        action = np.argmax(q[y, x, :])
        # currently chooses only the first action when tied. Probably better performance if this is random
    else:
        action = random.randrange(0, numberOfActions)
    return action


def SARSAEV(x, y, q, epsilon, numberOfActions):
    "finds the EV for ESARSA of the next action"
    expectation = 0
    for i in range(0, numberOfActions):
        expectation += ((epsilon / numberOfActions) * q[y, x, i])
    expectation += (1 - epsilon) * np.max(q[y, x, :])
    return expectation


def doARun(episodeMax, gamma, q, numberOfActions, stochastic, typeOfAlgorithm):
    "doARun performs a complete learning loop depending on which learner is asked for and returns scores"
    # iterate over episodeMax episodes
    scores = np.zeros((episodeMax-1, 1))
    canyonMap = np.tile(np.array([0, 0, 0, 1, 1, 1, 2, 2, 1, 0]), (7, 1))
    for i in range(1, episodeMax):
        # Initialize position
        x = 0
        y = 3
        terminate = False
        learningRate = 0.6 - (0.59/(episodeMax - i))
        epsilon = 0.15 - (0.001/(episodeMax - i))
        scoreCounter = 0

        # Pick an action given the state
        action = chooseAction(x, y, q, epsilon, numberOfActions)

        while not terminate:
            [xPrime, yPrime, reward, terminate] = takeStep(x, y, action, stochastic, canyonMap)
            if typeOfAlgorithm == 1:  # SARSA
                actionPrime = chooseAction(xPrime, yPrime, q, epsilon, numberOfActions)
                q[y, x, action] += learningRate * (reward + (gamma * q[yPrime, xPrime, actionPrime]) - q[y, x, action])
                action = actionPrime
            elif typeOfAlgorithm == 2:  # Q-learning
                q[y, x, action] += learningRate * (reward + (gamma * np.max(q[yPrime, xPrime, :]))  - q[y, x, action])
                action = chooseAction(x, y, q, epsilon, numberOfActions)
            elif typeOfAlgorithm == 3:  # ESARSA
                q[y, x, action] += learningRate * (reward + (gamma * SARSAEV(xPrime, yPrime, q, epsilon, numberOfActions)) - q[y, x, action])
                action = chooseAction(x, y, q, epsilon, numberOfActions)
            x = xPrime
            y = yPrime
            scoreCounter += 1
        scores[i-1] = scoreCounter
    # endfor
    return scores

def findPathName(numberOfActions, stochastic):
    "This function returns a path to the right file for saving and loading data of runs"
    
    workingLocation = os.path.dirname(__file__)
    if stochastic:
        if numberOfActions == 4:
            file = os.path.join(workingLocation,"scoresNewInitialization","scoreFourStoch.npy")
        elif numberOfActions == 8:
            file = os.path.join(workingLocation,"scoresNewInitialization","scoreEightStoch.npy")
        else:
            print("Stoch incorrect number of actions")
            return False
    else:
        if numberOfActions == 4:
            file = os.path.join(workingLocation,"scoresNewInitialization","scoreFourDeterm.npy")
        elif numberOfActions == 8:
            file = os.path.join(workingLocation,"scoresNewInitialization","scoreEightDeterm.npy")
        else:
            print("Determ incorrect number of actions")
            return False
    return file

def dataSaver(scores, numberOfActions, stochastic, typeOfAlgorithm):
    "Adds the scorestring to any existing data for later use"
    # The first  element of the array code the algorithstochastic, numberOfActions, position, startGraphs, titlem used, and we use different files for the type of game
    saveData = np.insert(scores, 0, [typeOfAlgorithm])
    file = findPathName(numberOfActions, stochastic)
       
    if os.path.exists(file):
        oldData = np.load(file)
        newData = np.insert(oldData, 0, saveData, axis=0)
        np.save(file, newData)
    else:
        np.save(file, np.expand_dims(saveData, axis=0))
    
    return True

def dataAggregator(numberOfActions, stochastic):
    "Returns a score array averaged over the different runs recorded for this configuration"
    file = findPathName(numberOfActions, stochastic)
    storedData = np.load(file)
    averagedScores = np.zeros((3, np.shape(storedData)[1]))
    for i in range (0, np.shape(storedData)[0]):
        typeOfAlgorithm = int(storedData[i,0])
        # Increment the number of entries averaged, stored in first column
        averagedScores[typeOfAlgorithm - 1, 0] += 1
        averagedScores[typeOfAlgorithm - 1, 1:] += storedData[i, 1:]
    for i in range(0,3):
        averagedScores[i, 1:] = averagedScores[i, 1:]/averagedScores[i,0]
    print(numberOfActions, stochastic," Number of runs averaged was %d for SARSA, %d for Q, %d for ESARSA"% (int(averagedScores[0,0]),int(averagedScores[1,0]), int(averagedScores[2,0])))
    return averagedScores

def graphDisplay(stochastic, numberOfActions, position, startGraphs, title):
    "Helps data display show each graph"
    
    averagedScores = dataAggregator(numberOfActions, stochastic)
    plt.subplot(position)
    plt.plot(range(startGraphs-1, np.shape(averagedScores)[1]-1), averagedScores[0, startGraphs:], "r-", label="SARSA")
    plt.plot(range(startGraphs-1, np.shape(averagedScores)[1]-1), averagedScores[1, startGraphs:], "b-", label="Q-learning")
    plt.plot(range(startGraphs-1, np.shape(averagedScores)[1]-1), averagedScores[2, startGraphs:], "g-", label="ESARSA")
    plt.title(title)
    plt.xlabel("Episodes")
    plt.ylabel("Steps")
    return True

def dataDisplay():
    "Displays the averaged runs for the different games, windStochasticity, and algorithms"
    
    plt.figure(1, figsize=(10,10))
    startGraphs = 10
    print("Graphs clipped to start at %d to show equilibrium performance"% (startGraphs) )
    graphDisplay(False, 4, 221, startGraphs, "4-action deterministic")
    graphDisplay(False, 8, 222, startGraphs, "8-action deterministic")
    graphDisplay(True, 4, 223, startGraphs, "4-action stochastic")
    graphDisplay(True, 8, 224, startGraphs, "8-action stochastic")
    plt.legend(loc='upper right')
    workingLocation = os.path.dirname(__file__)
    file = os.path.join(workingLocation,"outputDisplay.png")
    plt.savefig(file)
    plt.show()

# Now we do 10 runs of each configuration

for typeOfAlgorithm in range(1, 4):
    for stochastic in [True, False]:
        for numberOfActions in [4, 8]:
            for i in range(0, 10):
                q = np.ones((7, 10, numberOfActions)) * (-100)
                scores = doARun(300, 1, q, numberOfActions, stochastic, typeOfAlgorithm)
                dataSaver(scores, numberOfActions, stochastic, typeOfAlgorithm)

dataDisplay()
